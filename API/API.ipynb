{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "df_VWCE=pd.read_csv('Datasets/vwce.csv')\n",
    "df_SP500=pd.read_csv('Datasets/sp500.csv')\n",
    "df_APPLE=pd.read_csv('Datasets/apple.csv')\n",
    "df_MSCI=pd.read_csv('Datasets/msci.csv')\n",
    "df_NAS=pd.read_csv('Datasets/nasdaq.csv')\n",
    "df_EIMI=pd.read_csv(\"Datasets/eimi.csv\")\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Cleaning section. Merging to all-in-one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date conversion for every dataframe \n",
    "dfs = [df_SP500, df_MSCI, df_EIMI,df_APPLE, df_NAS]\n",
    "ordered_dfs = []\n",
    "\n",
    "for df in dfs:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    ordered_dfs.append(df.sort_values(by='Date', ascending=True).copy())\n",
    "\n",
    "df_SP500_ordered, df_MSCI_ordered, df_EIMI_ordered, df_APPLE_ordered, df_NAS_ordered = ordered_dfs\n",
    "\n",
    "#VWCE separated as it has a different date format\n",
    "df_VWCE_ordered = df_VWCE.copy() \n",
    "df_VWCE_ordered['Date'] = pd.to_datetime(df_VWCE_ordered['Date'])\n",
    "df_VWCE_ordered = df_VWCE_ordered.sort_values(by='Date', ascending=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_VWCE_ordered, df_SP500_ordered, df_MSCI_ordered, df_EIMI_ordered, df_APPLE_ordered, df_NAS_ordered]\n",
    "\n",
    "# Setting date as index\n",
    "for df in dfs:\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Removing % symbol and convert to float\n",
    "for df in dfs:\n",
    "    df['Change %'] = df['Change %'].replace('%', '', regex=True)  # Rimuove '%'\n",
    "    df['Change %'] = df['Change %'].astype(float)  # Converte in float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sp500 CSV uses ',' as separator, i need to replace it as i want a float value\n",
    "columns_to_edit = ['Price', 'Open', 'High', 'Low']\n",
    "for col in columns_to_edit:\n",
    "    df_SP500_ordered[col] = df_SP500_ordered[col].str.replace(',', '').astype(float)\n",
    "\n",
    "columns_to_edit = ['Price', 'Open', 'High', 'Low']\n",
    "for col in columns_to_edit:\n",
    "    df_NAS_ordered[col] = df_NAS_ordered[col].str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "dfs2 = [df_SP500_ordered, df_NAS_ordered, df_EIMI_ordered, df_APPLE_ordered, df_MSCI_ordered]\n",
    "columns = ['Price', 'Price', 'Price', 'Price', 'Price']  # Colonna 'Price' per tutti tranne MSCI\n",
    "\n",
    "for i, df in enumerate(dfs2):\n",
    "    # Normalize prices\n",
    "    df['Normalized Price'] = scaler.fit_transform(df[[columns[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_SP500_ordered[['Normalized Price']].merge(\n",
    "    df_NAS_ordered[['Normalized Price']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('_SP500', '_NAS')\n",
    ")\n",
    "\n",
    "df_merged = df_merged.merge(\n",
    "    df_EIMI_ordered[['Normalized Price']], \n",
    "    left_index=True, \n",
    "    right_index=True,\n",
    "    suffixes=('', '_EIMI')\n",
    ")\n",
    "\n",
    "df_merged = df_merged.merge(\n",
    "    df_APPLE_ordered[['Normalized Price']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('', '_APPLE')\n",
    ")\n",
    "\n",
    "df_merged = df_merged.merge(\n",
    "    df_MSCI_ordered[['Normalized Price']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('', '_MSCI')\n",
    ")\n",
    "\n",
    "df_merged = df_merged.rename(columns={'Normalized Price': 'Normalized Price_EIMI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2 = df_SP500_ordered[['Price']].merge(\n",
    "    df_NAS_ordered[['Price']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('_SP500', '_NAS')\n",
    ")\n",
    "\n",
    "df_merged2 = df_merged2.merge(\n",
    "    df_EIMI_ordered[['Price']], \n",
    "    left_index=True, \n",
    "    right_index=True,\n",
    "    suffixes=('', '_EIMI')\n",
    ")\n",
    "\n",
    "df_merged2 = df_merged2.merge(\n",
    "    df_APPLE_ordered[['Price']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('', '_APPLE')\n",
    ")\n",
    "\n",
    "df_merged2 = df_merged2.merge(\n",
    "    df_MSCI_ordered[['Price']], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('', '_MSCI')\n",
    ")\n",
    "\n",
    "df_merged2 = df_merged2.rename(columns={'Price': 'Price_EIMI'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Systems with deeplearning: MLP and LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 3525.8474\n",
      "Epoch [20/100], Loss: 624.0200\n",
      "Epoch [30/100], Loss: 1339.1064\n",
      "Epoch [40/100], Loss: 1426.8162\n",
      "Epoch [50/100], Loss: 704.7308\n",
      "Epoch [60/100], Loss: 254.9856\n",
      "Epoch [70/100], Loss: 265.4350\n",
      "Epoch [80/100], Loss: 218.7416\n",
      "Epoch [90/100], Loss: 206.1499\n",
      "Epoch [100/100], Loss: 201.0036\n",
      "Prediction: 229.28\n"
     ]
    }
   ],
   "source": [
    "#Some deep learning: MLP\n",
    "\n",
    "X = df_merged2[['Price_NAS', 'Price_SP500']].values  # Input\n",
    "y = df_merged2['Price_APPLE'].values  # Target \n",
    "\n",
    "# Convert to tensor\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Target deve essere una colonna\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)  #input\n",
    "        self.fc2 = nn.Linear(64, 32)  #hidden layer\n",
    "        self.fc3 = nn.Linear(32, 1)   # Un output\n",
    "        self.dropout = nn.Dropout(0.3)  # 30% dei neuroni disattivati\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "model = SimpleModel()\n",
    "\n",
    "# Loss and optimization function\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Backpropagation and optiomization\n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()  \n",
    "    optimizer.step() \n",
    "\n",
    "    if (epoch + 1) % 10 == 0:  # print loss value\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "prezzo1_input = 21150\n",
    "prezzo2_input = 6050\n",
    "\n",
    "\n",
    "input_data = torch.tensor([[prezzo1_input, prezzo2_input]], dtype=torch.float32)\n",
    "\n",
    "#prediction\n",
    "model.eval() \n",
    "predizione = model(input_data)\n",
    "\n",
    "print(f\"Prediction: {predizione.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 13447.5039\n",
      "Epoch [20/500], Loss: 13384.4785\n",
      "Epoch [30/500], Loss: 13315.4307\n",
      "Epoch [40/500], Loss: 13230.8818\n",
      "Epoch [50/500], Loss: 13122.5166\n",
      "Epoch [60/500], Loss: 12984.2949\n",
      "Epoch [70/500], Loss: 12809.5586\n",
      "Epoch [80/500], Loss: 12590.5215\n",
      "Epoch [90/500], Loss: 12313.7646\n",
      "Epoch [100/500], Loss: 11972.7236\n",
      "Epoch [110/500], Loss: 11565.1748\n",
      "Epoch [120/500], Loss: 11080.1299\n",
      "Epoch [130/500], Loss: 10507.9092\n",
      "Epoch [140/500], Loss: 9858.3496\n",
      "Epoch [150/500], Loss: 9138.3936\n",
      "Epoch [160/500], Loss: 8358.1953\n",
      "Epoch [170/500], Loss: 7531.8892\n",
      "Epoch [180/500], Loss: 6677.5381\n",
      "Epoch [190/500], Loss: 5816.5933\n",
      "Epoch [200/500], Loss: 4972.8740\n",
      "Epoch [210/500], Loss: 4170.9263\n",
      "Epoch [220/500], Loss: 3434.0347\n",
      "Epoch [230/500], Loss: 2781.8418\n",
      "Epoch [240/500], Loss: 2228.2424\n",
      "Epoch [250/500], Loss: 1779.3334\n",
      "Epoch [260/500], Loss: 1432.9045\n",
      "Epoch [270/500], Loss: 1179.2866\n",
      "Epoch [280/500], Loss: 1003.2570\n",
      "Epoch [290/500], Loss: 886.3754\n",
      "Epoch [300/500], Loss: 810.4812\n",
      "Epoch [310/500], Loss: 760.6133\n",
      "Epoch [320/500], Loss: 725.8843\n",
      "Epoch [330/500], Loss: 699.1866\n",
      "Epoch [340/500], Loss: 676.5217\n",
      "Epoch [350/500], Loss: 656.0789\n",
      "Epoch [360/500], Loss: 636.6879\n",
      "Epoch [370/500], Loss: 617.6764\n",
      "Epoch [380/500], Loss: 598.8595\n",
      "Epoch [390/500], Loss: 580.1866\n",
      "Epoch [400/500], Loss: 561.6630\n",
      "Epoch [410/500], Loss: 543.3636\n",
      "Epoch [420/500], Loss: 525.2811\n",
      "Epoch [430/500], Loss: 507.4762\n",
      "Epoch [440/500], Loss: 490.1348\n",
      "Epoch [450/500], Loss: 473.3179\n",
      "Epoch [460/500], Loss: 457.0692\n",
      "Epoch [470/500], Loss: 441.3928\n",
      "Epoch [480/500], Loss: 426.2763\n",
      "Epoch [490/500], Loss: 411.7402\n",
      "Epoch [500/500], Loss: 397.7980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# 1. Estrai i dati\n",
    "X = df_merged2[['Price_NAS', 'Price_SP500']].values\n",
    "y = df_merged2['Price_APPLE'].values\n",
    "\n",
    "# 2. Normalizza gli input\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Tensor\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 4. Modello\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Training loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 6. Salva il modello e lo scaler\n",
    "torch.save(model.state_dict(), 'mlp_model.pth')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['c:\\\\Users\\\\chris\\\\Desktop\\\\Financial-Analytics']\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [8512] using StatReload\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "uvicorn.run(\"app:app\", host=\"127.0.0.1\", port=8000, reload=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
